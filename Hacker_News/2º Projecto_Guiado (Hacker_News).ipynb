{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analysing Hacker_News Dataset (2nd Project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this Project we will be exploring a Dataset from the famous Hacker News site. We will be analyzing some data from their community, mainly the posts submited by their users that target the Ask HN and Show HN options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at'], ['12224879', 'Interactive Dynamic Video', 'http://www.interactivedynamicvideo.com/', '386', '52', 'ne0phyte', '8/4/2016 11:52'], ['10975351', 'How to Use Open Source and Shut the Fuck Up at the Same Time', 'http://hueniverse.com/2016/01/26/how-to-use-open-source-and-shut-the-fuck-up-at-the-same-time/', '39', '10', 'josep2', '1/26/2016 19:30'], ['11964716', \"Florida DJs May Face Felony for April Fools' Water Joke\", 'http://www.thewire.com/entertainment/2013/04/florida-djs-april-fools-water-joke/63798/', '2', '1', 'vezycash', '6/23/2016 22:20'], ['11919867', 'Technology ventures: From Idea to Enterprise', 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429', '3', '1', 'hswarna', '6/17/2016 0:01']]\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "\n",
    "hn_n = open('hacker_news.csv')\n",
    "hn = reader(hn_n)\n",
    "hn = list(hn)\n",
    "\n",
    "print(hn[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting and 'deleting' the first row from our Dataset. That corresponds to the Header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at']\n",
      "\n",
      "\n",
      "[['12224879', 'Interactive Dynamic Video', 'http://www.interactivedynamicvideo.com/', '386', '52', 'ne0phyte', '8/4/2016 11:52'], ['10975351', 'How to Use Open Source and Shut the Fuck Up at the Same Time', 'http://hueniverse.com/2016/01/26/how-to-use-open-source-and-shut-the-fuck-up-at-the-same-time/', '39', '10', 'josep2', '1/26/2016 19:30'], ['11964716', \"Florida DJs May Face Felony for April Fools' Water Joke\", 'http://www.thewire.com/entertainment/2013/04/florida-djs-april-fools-water-joke/63798/', '2', '1', 'vezycash', '6/23/2016 22:20'], ['11919867', 'Technology ventures: From Idea to Enterprise', 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429', '3', '1', 'hswarna', '6/17/2016 0:01'], ['10301696', 'Note by Note: The Making of Steinway L1037 (2007)', 'http://www.nytimes.com/2007/11/07/movies/07stein.html?_r=0', '8', '2', 'walterbell', '9/30/2015 4:12']]\n",
      "\n",
      "\n",
      "20100\n"
     ]
    }
   ],
   "source": [
    "headers = hn[0]\n",
    "hn = hn[1:]\n",
    "print(headers)\n",
    "print('\\n')\n",
    "print(hn[0:5])\n",
    "print('\\n')\n",
    "print(len(hn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Disaggregating and separating our Dataset into 3 new different Dataset's: Ask_HN; Show_HN and Other posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1744\n",
      "1162\n",
      "17194\n"
     ]
    }
   ],
   "source": [
    "ask_posts = []\n",
    "show_posts = []\n",
    "other_posts = []\n",
    "\n",
    "for row in hn:\n",
    "    title = row[1]\n",
    "    if title.lower().startswith('ask hn'):\n",
    "        ask_posts.append(row)\n",
    "    elif title.lower().startswith('show hn'):\n",
    "        show_posts.append(row)\n",
    "    else:\n",
    "        other_posts.append(row)\n",
    "\n",
    "print(len(ask_posts))\n",
    "print(len(show_posts))\n",
    "print(len(other_posts))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['12296411', 'Ask HN: How to improve my personal website?', '', '2', '6', 'ahmedbaracat', '8/16/2016 9:55'], ['10610020', 'Ask HN: Am I the only one outraged by Twitter shutting down share counts?', '', '28', '29', 'tkfx', '11/22/2015 13:43'], ['11610310', 'Ask HN: Aby recent changes to CSS that broke mobile?', '', '1', '1', 'polskibus', '5/2/2016 10:14'], ['12210105', 'Ask HN: Looking for Employee #3 How do I do it?', '', '1', '3', 'sph130', '8/2/2016 14:20'], ['10394168', 'Ask HN: Someone offered to buy my browser extension from me. What now?', '', '28', '17', 'roykolak', '10/15/2015 16:38']]\n",
      "[['10627194', 'Show HN: Wio Link  ESP8266 Based Web of Things Hardware Development Platform', 'https://iot.seeed.cc', '26', '22', 'kfihihc', '11/25/2015 14:03'], ['10646440', 'Show HN: Something pointless I made', 'http://dn.ht/picklecat/', '747', '102', 'dhotson', '11/29/2015 22:46'], ['11590768', 'Show HN: Shanhu.io, a programming playground powered by e8vm', 'https://shanhu.io', '1', '1', 'h8liu', '4/28/2016 18:05'], ['12178806', 'Show HN: Webscope  Easy way for web developers to communicate with Clients', 'http://webscopeapp.com', '3', '3', 'fastbrick', '7/28/2016 7:11'], ['10872799', 'Show HN: GeoScreenshot  Easily test Geo-IP based web pages', 'https://www.geoscreenshot.com/', '1', '9', 'kpsychwave', '1/9/2016 20:45']]\n"
     ]
    }
   ],
   "source": [
    "#first 5 rows from ask_posts:\n",
    "print(ask_posts[:5])\n",
    "\n",
    "#first 5 rows from show_posts:\n",
    "print(show_posts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask_Comments_Avg: 14.038417431192661\n",
      "Show_Comments_Avg: 10.31669535283993\n"
     ]
    }
   ],
   "source": [
    "#Calculating the average comments in the ask_comments section:\n",
    "total_ask_comments = 0\n",
    "\n",
    "for row in ask_posts:\n",
    "    num_comments = int(row[4])   \n",
    "    total_ask_comments += num_comments\n",
    "\n",
    "avg_ask_comments = total_ask_comments/len(ask_posts)\n",
    "print('Ask_Comments_Avg:', avg_ask_comments)\n",
    "\n",
    "\n",
    "#Calculating the average comments in the show_comments section:\n",
    "total_show_comments = 0\n",
    "\n",
    "for row in show_posts:\n",
    "    num_comments = int(row[4])\n",
    "    total_show_comments += num_comments\n",
    "    \n",
    "avg_show_comments = total_show_comments/len(show_posts)\n",
    "print('Show_Comments_Avg:', avg_show_comments)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On average, Ask_Posts receive 4 more comments per post than its counterpart Show_Posts! (14.03 Vs. 10.32)\n",
    "##### This can be because a question thread makes more buzz than any other. Piling up different opinions from different users, and a kind of argumentative battle between all of them, boosting up comments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the next step we will create two dictionaries. One for the frequency of ask_posts by hour, and another one for the frequency of comments by hour. This task is to be able to calculate the average of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'02': 58, '04': 47, '18': 109, '23': 68, '09': 45, '19': 110, '05': 46, '13': 85, '06': 44, '20': 80, '17': 100, '16': 108, '07': 34, '08': 48, '10': 59, '15': 116, '22': 71, '12': 73, '03': 54, '14': 107, '11': 58, '01': 60, '00': 55, '21': 109}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'02': 1381, '04': 337, '18': 1439, '23': 543, '09': 251, '19': 1188, '05': 464, '13': 1253, '06': 397, '20': 1722, '17': 1146, '16': 1814, '07': 267, '08': 492, '10': 793, '15': 4477, '22': 479, '12': 687, '03': 421, '14': 1416, '11': 641, '01': 683, '00': 447, '21': 1745}\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "#Looping through the ask_posts list, and creating a new\n",
    "#list of lists, using the append method and inside the\n",
    "#append method instead of using one object(a single list,\n",
    "#or a sinle variable) we use 2 and wrap them up in a \n",
    "#single list ([]). So that when the append method works\n",
    "#it will append that list inside another empty list\n",
    "#(result_list), creating a list of lists:\n",
    "result_list = []\n",
    "\n",
    "for row in ask_posts:\n",
    "    created_at = row[6]\n",
    "    num_comments = row[4]\n",
    "    result_list.append([created_at, num_comments])\n",
    "\n",
    "#print(result_list)\n",
    "\n",
    "#Another way of doing the same things as above, is by \n",
    "#using the List Comprehension method:\n",
    "\n",
    "result_list_2 = [[row[6], row[4]] for row in ask_posts]\n",
    "#print(result_list_2)\n",
    "\n",
    "#The above method is a more elegant and Pythonic way of\n",
    "#doing it! Here we consume less memory and write less \n",
    "#code, being less prone to mistakes! \n",
    "\n",
    "counts_by_hour = {}\n",
    "comments_by_hour = {}\n",
    "\n",
    "for row in result_list:\n",
    "    hour_date = row[0]\n",
    "    comments = int(row[1])\n",
    "    hour_date_int = dt.datetime.strptime(hour_date, '%m/%d/%Y %H:%M')\n",
    "    hour = hour_date_int.strftime('%H')\n",
    "    if hour not in counts_by_hour:\n",
    "        counts_by_hour[hour] = 1\n",
    "        comments_by_hour[hour] = comments\n",
    "    else:\n",
    "        counts_by_hour[hour] += 1\n",
    "        comments_by_hour[hour] += comments\n",
    "        \n",
    "print(counts_by_hour)\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print(comments_by_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now its time to create a list of lists displaying the average number of comments per post in each hour! \n",
    "##### Our lists will have two elements each, being the first the hour, and the second the average between the number of comments and the number of posts. The first element(hour) will act as the key and the second(average) as its value, just like in a dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['02', 23.810344827586206], ['04', 7.170212765957447], ['18', 13.20183486238532], ['23', 7.985294117647059], ['09', 5.5777777777777775], ['19', 10.8], ['05', 10.08695652173913], ['13', 14.741176470588234], ['06', 9.022727272727273], ['20', 21.525], ['17', 11.46], ['16', 16.796296296296298], ['07', 7.852941176470588], ['08', 10.25], ['10', 13.440677966101696], ['15', 38.5948275862069], ['22', 6.746478873239437], ['12', 9.41095890410959], ['03', 7.796296296296297], ['14', 13.233644859813085], ['11', 11.051724137931034], ['01', 11.383333333333333], ['00', 8.127272727272727], ['21', 16.009174311926607]]\n"
     ]
    }
   ],
   "source": [
    "avg_by_hour = []\n",
    "\n",
    "for hour in comments_by_hour:\n",
    "    avg_by_hour.append([hour, (int(comments_by_hour[hour])/counts_by_hour[hour])])\n",
    "\n",
    "print(avg_by_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Although a bit confusing at first glanse, we can already see from this analysis that the hour of the day that receives more Comments per Post (ask_post) is the 15th hour. With an average number of approximatly 39 comments per post, 3 o'clock ranks far ahead at number 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We should work on our avg_by_hour list of lists, in order to make it more clean, and more user friendly. But also to present its results in a better readable way!\n",
    "##### We are going to do this first by creating a new list were we swap the columns of our avg_by_hour list of lists, then we will sort its lists, and present the first five results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23.810344827586206, '02'], [7.170212765957447, '04'], [13.20183486238532, '18'], [7.985294117647059, '23'], [5.5777777777777775, '09'], [10.8, '19'], [10.08695652173913, '05'], [14.741176470588234, '13'], [9.022727272727273, '06'], [21.525, '20'], [11.46, '17'], [16.796296296296298, '16'], [7.852941176470588, '07'], [10.25, '08'], [13.440677966101696, '10'], [38.5948275862069, '15'], [6.746478873239437, '22'], [9.41095890410959, '12'], [7.796296296296297, '03'], [13.233644859813085, '14'], [11.051724137931034, '11'], [11.383333333333333, '01'], [8.127272727272727, '00'], [16.009174311926607, '21']]\n",
      "\n",
      "\n",
      "Top 5 Hours for Ask Posts Comments:\n",
      "15:00: 38.59 average comments per post\n",
      "02:00: 23.81 average comments per post\n",
      "20:00: 21.52 average comments per post\n",
      "16:00: 16.80 average comments per post\n",
      "21:00: 16.01 average comments per post\n"
     ]
    }
   ],
   "source": [
    "swap_avg_by_hour = []\n",
    "\n",
    "# Using a for loop to iterate over the elements of the avg_by_hour variable\n",
    "# and append those elemenst into a new list of lists, swap_avg_by_hour, but\n",
    "# with the original columns swaped:\n",
    "for row in avg_by_hour:\n",
    "    swap_avg_by_hour.append([row[1], row[0]])\n",
    "    \n",
    "print(swap_avg_by_hour)\n",
    "print('\\n')\n",
    "\n",
    "# Sorting through the swap_avg_by_hour, and doing it in descending order:\n",
    "sorted_swap = sorted(swap_avg_by_hour, reverse=True)\n",
    "#print(sorted_swap)\n",
    "\n",
    "# Presenting the top 5 results of the sorted list, and presenting them using\n",
    "# the str.format() method, were the first element is the hour, and the second\n",
    "# the average of comments received per hour, presented in to 2 decimal places:\n",
    "print(\"Top 5 Hours for Ask Posts Comments:\")\n",
    "for row in sorted_swap[:5]:\n",
    "    avg_comments = row[0]\n",
    "    h = row[1]\n",
    "    h = dt.datetime.strptime(h, '%H')\n",
    "    h_str = h.strftime('%H:%M')\n",
    "    comments_h_str = '{}: {:.2f} average comments per post'. format(h_str, avg_comments)\n",
    "    print(comments_h_str)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After the analysis we can clearly tell that the best hour of the day to create an ask_post, in order to have a higher chance of receiving comments, would be around 3p.m. With an average of approximately 39 comments per post its by far the best time of the day. Followed by 02:00, with roughly 24 comments per post, and then 20:00 with 21 comments. \n",
    "### These hours all come in the timezone of the DataSet, that is EST USA (UTC/GMT -4). If we want to put it in our/mine timezone (GMT+1), we should had 5 more hours to it. So the results for us, sharing the same timezone, should be 20:00 @#1; 07:00 @#2; 01:00 @#3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
